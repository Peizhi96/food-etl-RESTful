# Starter pipeline
# Start with a minimal pipeline that you can customize to build and deploy your code.
# Add steps that build, run tests, deploy, and more:
# https://aka.ms/yaml

trigger:
- main

pool:
  vmImage: ubuntu-latest

variables:
  DATABRICKS_HOST: 'https://adb-750644874697514.14.azuredatabricks.net'

steps:
- checkout: self

- script: |
    sudo apt-get update
    sudo apt-get install -y unixodbc-dev libpq-dev
  displayName: 'Install system dependencies'

- task: UsePythonVersion@0
  inputs:
    versionSpec: '3.x'
- script: |
    pip install -r requirements.txt
    pip install databricks-cli
  displayName: 'Install Python dependencies'

- script: |
    export DATABRICKS_HOST=https://adb-750644874697514.14.azuredatabricks.net
    export DATABRICKS_TOKEN=$(DATABRICKS_PAT)

    echo "[DEFAULT]" > ~/.databrickscfg
    echo "host = $DATABRICKS_HOST" >> ~/.databrickscfg
    echo "token = $DATABRICKS_TOKEN" >> ~/.databrickscfg

    echo "Listing contents of current directory:"
    ls -la

    echo "Host: $DATABRICKS_HOST"
    echo "Token: ${DATABRICKS_TOKEN:0:4}****"
    
    databricks workspace import --language PYTHON --overwrite ./restaurant.ipynb /Shared/restaurant.ipynb
  displayName: 'Deploy Notebooks to Databricks'
  env:
    DATABRICKS_PAT: $(DATABRICKS_PAT)
